{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poornima-bhatia/ES_335_Machine_Learning/blob/main/assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9jwyJEHeVpUg"
      },
      "outputs": [],
      "source": [
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from torch import optim, cuda\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from PIL import Image\n",
        "import warnings\n",
        "from time import time\n",
        "warnings.filterwarnings('ignore', category = FutureWarning)\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icDniktJYFAS",
        "outputId": "1453fb19-dafa-4030-b8d7-1d9830397227"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eiGbHkJ7Vysm",
        "outputId": "1ae521ed-aed6-4c86-f731-0cf1b91abe86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traindir = '/content/drive/MyDrive/ML_assignment4/train/'\n",
        "testdir ='/content/drive/MyDrive/ML_assignment4/test/'\n",
        "categories = []\n",
        "img_categories = []\n",
        "n_train = []\n",
        "n_test = []\n",
        "hs = []\n",
        "ws = []\n",
        "# Iterate through each category\n",
        "for d in os.listdir(traindir):\n",
        "  categories.append(d)\n",
        "    # Number of each image\n",
        "  train_imgs = os.listdir(traindir + d)\n",
        "  test_imgs = os.listdir(testdir + d)\n",
        "  n_train.append(len(train_imgs))\n",
        "  n_test.append(len(test_imgs))\n",
        "\n",
        "    # # Find stats for train images\n",
        "    # for i in train_imgs:\n",
        "    #     img_categories.append(d)\n",
        "    #     img = Image.open(traindir + d + '/' + i)\n",
        "    #     img_array = np.array(img)\n",
        "    #     # Shape\n",
        "    #     hs.append(img_array.shape[0])\n",
        "    #     ws.append(img_array.shape[1])"
      ],
      "metadata": {
        "id": "GZ9Fu1nPV0ZN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "3a6d015a-32c2-45b2-aff7-f5793bef677e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "[Errno 20] Not a directory: '/content/drive/MyDrive/ML_assignment4/train/pianos11.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d08c40acfb08>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mcategories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Number of each image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mtrain_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mtest_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mn_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/drive/MyDrive/ML_assignment4/train/pianos11.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe of categories\n",
        "cat_df = pd.DataFrame({'category': categories,\n",
        "                       'n_train': n_train,\n",
        "                        'n_test': n_test}).\\\n",
        "    sort_values('category')\n",
        "print(cat_df)\n",
        "\n",
        "\n",
        "image_df = pd.DataFrame({\n",
        "    'category': img_categories,\n",
        "    'height': hs,\n",
        "    'width': ws\n",
        "})\n",
        "print(image_df)"
      ],
      "metadata": {
        "id": "d4roJKslV-qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "LffK_x9mWGHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets from each folder\n",
        "images = {\n",
        "    'train':\n",
        "    datasets.ImageFolder(root=traindir, transform= simple_transform ),\n",
        "    'test':\n",
        "    datasets.ImageFolder(root=testdir, transform= simple_transform)\n",
        "}\n",
        "\n",
        "batch_size = 1\n",
        "# Dataloader iterators\n",
        "dataloaders = {\n",
        "    'train': DataLoader(torch.utils.data.Subset(images['train'], range(0, 140)), batch_size=batch_size, shuffle=True),\n",
        "    'val' : DataLoader(torch.utils.data.Subset(images['train'], range(140, 160)), batch_size=batch_size, shuffle=True),\n",
        "    'test': DataLoader(images['test'], batch_size=batch_size, shuffle=True)\n",
        "}"
      ],
      "metadata": {
        "id": "646AbNK2WJ_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image , label in dataloaders['train']:\n",
        "    print(image.shape)\n",
        "    print(label)\n",
        "    print(label.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "nVRUfSYVWL9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(cat_df)\n",
        "print(f'There are {n_classes} different classes.')\n",
        "\n",
        "len(images['train'].classes)"
      ],
      "metadata": {
        "id": "tlbRA83EWMjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model():\n",
        "    model = nn.Sequential(\n",
        "      nn.Conv2d(3, 32, 3), # 3 input channel, 32 filters of size 3x3\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(32*111*111 , 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 2)\n",
        "      )\n",
        "    return model"
      ],
      "metadata": {
        "id": "R1fiBXxH0KCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg1 = define_model()"
      ],
      "metadata": {
        "id": "IK3fbD1f0L-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEVICE == 'cuda':\n",
        "  vgg1.to('cuda')"
      ],
      "metadata": {
        "id": "ba543_C30P27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(vgg1, input_size = (3, 224, 224))"
      ],
      "metadata": {
        "id": "GEe1m95H0Smi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg1.class_to_idx =  images['train'].class_to_idx\n",
        "vgg1.idx_to_class = {\n",
        "    idx: class_\n",
        "    for class_, idx in vgg1.class_to_idx.items()\n",
        "}\n",
        "\n",
        "list(vgg1.idx_to_class.items())[:10]"
      ],
      "metadata": {
        "id": "NeZ-FEDt0pxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in vgg1.parameters())\n",
        "total_params"
      ],
      "metadata": {
        "id": "WKTZ3Bju0v6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vgg1.parameters())\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracy = []\n",
        "valid_accuracy = []\n",
        "training_time = []\n",
        "\n",
        "writer = SummaryWriter(f'runs/vgg1/')\n",
        "step = 0\n",
        "overall_start_time = time()\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    train_acc = 0\n",
        "    valid_acc = 0\n",
        "\n",
        "    # Train the model\n",
        "    vgg1.train()\n",
        "    start_time = time()\n",
        "    for data, target in dataloaders['train']:\n",
        "        if DEVICE == 'cuda':\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "        # print(target.shape) #torch.Size([160])\n",
        "        # print(data.shape) #torch.Size([160, 3, 224, 224])\n",
        "        optimizer.zero_grad()\n",
        "        output = vgg1(data) # output has probabilities of each class while the target have index of the class that is why class to index mapping is important\n",
        "        # print(output.shape) #torch.Size([160, 2])\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "        pred = torch.argmax(input = output, dim = 1)\n",
        "        correct_labels = pred.eq(target.data.view_as(pred))\n",
        "        acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "        train_acc += acc.item() * data.size(0)\n",
        "\n",
        "        writer.add_scalar('Trainig Loss/ Iteration', loss, global_step = step)\n",
        "        writer.add_scalar('Training Accuracy / Iteration', acc, global_step = step)\n",
        "        step += 1\n",
        "\n",
        "    train_time = time() - start_time\n",
        "    print(f'Training time in {epoch+1} is {train_time}\\n')\n",
        "\n",
        "    # Evaluate the model\n",
        "    vgg1.eval()\n",
        "    for data, target in dataloaders['val']:\n",
        "        if DEVICE == 'cuda':\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "        output = vgg1(data)\n",
        "        # print(output)\n",
        "        loss = criterion(output, target)\n",
        "        val_loss += loss.item()*data.size(0)\n",
        "\n",
        "        pred = torch.argmax(input = output, dim = 1)\n",
        "        correct_labels = pred.eq(target.data.view_as(pred))\n",
        "        acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "        valid_acc += acc.item() * data.size(0)\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate average losses\n",
        "    train_loss = train_loss/len(dataloaders['train'].sampler)\n",
        "    val_loss = val_loss/len(dataloaders['val'].sampler)\n",
        "    train_acc = train_acc / len(dataloaders['train'].sampler)\n",
        "    valid_acc = valid_acc / len(dataloaders['val'].sampler)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracy.append(train_acc)\n",
        "    valid_accuracy.append(valid_acc)\n",
        "    training_time.append(train_time)\n",
        "\n",
        "    # Print training/validation statistics\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch+1,\n",
        "        train_loss,\n",
        "        val_loss\n",
        "        ))\n",
        "\n",
        "\n",
        "overall_train_time = time() - overall_start_time\n",
        "print(f'Overall time taken for {epoch} including training and validation {overall_train_time}')"
      ],
      "metadata": {
        "id": "BxPWsaTB038F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg1_acc_loss_time = pd.DataFrame([], columns = ['TrainingLoss','ValidationLoss', 'TrainingTime', 'TrainingAccuracy','ValidAccuracy', 'TestAccuracy','TotalParameters'  ])\n",
        "vgg1_acc_loss_time = pd.DataFrame({'TrainingLoss' : train_losses,\n",
        "                              'ValidationLoss' : val_losses,\n",
        "                              'TrainingTime' : training_time,\n",
        "                              'TrainingAccuracy' : train_accuracy,\n",
        "                              'ValidAccuracy' : valid_accuracy,\n",
        "                              'TotalParameters' : np.repeat(total_params, 10) # 10 is number of epochs\n",
        "                              })\n",
        "vgg1_acc_loss_time"
      ],
      "metadata": {
        "id": "9GOE1ovg1DSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig , ax =plt.subplots(nrows=1,ncols =2, figsize = (15,5))\n",
        "ax[0].plot( range(10), vgg1_acc_loss_time['TrainingLoss'], label= 'Training Loss')\n",
        "ax[0].plot( range(10), vgg1_acc_loss_time['ValidationLoss'], label= 'Validation Loss')\n",
        "ax[1].plot( range(10), vgg1_acc_loss_time['TrainingAccuracy'], label= 'Training Accuracy')\n",
        "ax[1].plot( range(10), vgg1_acc_loss_time['ValidAccuracy'], label= 'Validation Accuracy')\n",
        "ax[0].legend()\n",
        "ax[1].legend()"
      ],
      "metadata": {
        "id": "Q9NDAXyT1MaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = 0\n",
        "step = 0\n",
        "\n",
        "for input, target in dataloaders['test']:\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  if DEVICE == 'cuda':\n",
        "          input, target = input.cuda(), target.cuda()\n",
        "  output = vgg1(input)\n",
        "  pred = torch.argmax(input = output, dim = 1)\n",
        "  correct_labels = pred.eq(target.data.view_as(pred))\n",
        "  acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "  test_acc += acc.item()* input.size(0)\n",
        "\n",
        "  ax.imshow(input.cpu().squeeze().reshape(224,224,3))\n",
        "  ax.set_title(f'Class : {vgg1.idx_to_class[pred.item()]}')\n",
        "  ax.set_axis_off()\n",
        "\n",
        "  writer.add_scalar('Testing Accuracy / Iteration', acc, global_step = step)\n",
        "  writer.add_figure('Test images & Label', fig, global_step = step)\n",
        "  step +=1\n",
        "\n",
        "test_acc = test_acc / len(dataloaders['test'].sampler)\n",
        "vgg1_acc_loss_time['TestAccuracy'] = np.repeat(test_acc, len(vgg1_acc_loss_time))\n",
        "print(f'Testing accuracy is {test_acc}')\n"
      ],
      "metadata": {
        "id": "kbKlnjB11i7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "fgNcQHYJ1UO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs/vgg1"
      ],
      "metadata": {
        "id": "xPNO8Wp81XB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input, target in dataloaders['test']:\n",
        "  if DEVICE == 'cuda':\n",
        "          input, target = input.cuda(), target.cuda()\n",
        "  # print(input.shape)\n",
        "  result = vgg1(input)\n",
        "  pred_class = torch.argmax(result).item()\n",
        "  plt.title(f'Predicted class is {vgg1.idx_to_class[pred_class]}')\n",
        "  plt.imshow(input.cpu().squeeze().reshape(224,224,3))\n",
        "  plt.xlim([0,100])\n",
        "  plt.ylim([0,100])\n",
        "  break\n"
      ],
      "metadata": {
        "id": "Dqpc-Eo518WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG 3"
      ],
      "metadata": {
        "id": "_IMlBvJK43nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model():\n",
        "  model = nn.Sequential(\n",
        "      nn.Conv2d(3, 32, 3), # 3 input channel, 32 filters of size 3x3\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.Conv2d(32, 64, 3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.Conv2d(64, 128, 3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(26 * 26 *128 , 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 2)\n",
        "  )\n",
        "  return model"
      ],
      "metadata": {
        "id": "-LWqmpIMWOUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg3 = define_model()"
      ],
      "metadata": {
        "id": "JqJNJ2DZdZG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEVICE == 'cuda':\n",
        "  vgg3.to('cuda')"
      ],
      "metadata": {
        "id": "G25ZhMCUZMd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(vgg3, input_size = (3, 224, 224))"
      ],
      "metadata": {
        "id": "qMQDhaG-dtQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg3.class_to_idx =  images['train'].class_to_idx\n",
        "vgg3.idx_to_class = {\n",
        "    idx: class_\n",
        "    for class_, idx in vgg3.class_to_idx.items()\n",
        "}\n",
        "\n",
        "list(vgg3.idx_to_class.items())[:10]"
      ],
      "metadata": {
        "id": "kwzUJ9dECu_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Training time\n",
        "- Training loss\n",
        "- Training accuracy\n",
        "- Testing accuracy\n",
        "- Number of model parameters"
      ],
      "metadata": {
        "id": "ytt5vyLTyncR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in vgg3.parameters())\n",
        "total_params"
      ],
      "metadata": {
        "id": "oMdMPz2V3uoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vgg3.parameters())\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracy = []\n",
        "valid_accuracy = []\n",
        "training_time = []\n",
        "\n",
        "writer = SummaryWriter(f'runs/vgg3/')\n",
        "step = 0\n",
        "overall_start_time = time()\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    train_acc = 0\n",
        "    valid_acc = 0\n",
        "\n",
        "    # Train the model\n",
        "    vgg3.train()\n",
        "    start_time = time()\n",
        "    for data, target in dataloaders['train']:\n",
        "        if DEVICE == 'cuda':\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "        # print(target.shape) #torch.Size([160])\n",
        "        # print(data.shape) #torch.Size([160, 3, 224, 224])\n",
        "        optimizer.zero_grad()\n",
        "        output = vgg3(data) # output has probabilities of each class while the target have index of the class that is why class to index mapping is important\n",
        "        # print(output.shape) #torch.Size([160, 2])\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "        pred = torch.argmax(input = output, dim = 1)\n",
        "        correct_labels = pred.eq(target.data.view_as(pred))\n",
        "        acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "        train_acc += acc.item() * data.size(0)\n",
        "\n",
        "        writer.add_scalar('Trainig Loss/ Iteration', loss, global_step = step)\n",
        "        writer.add_scalar('Training Accuracy / Iteration', acc, global_step = step)\n",
        "        step += 1\n",
        "\n",
        "    train_time = time() - start_time\n",
        "    print(f'Training time in {epoch+1} is {train_time}\\n')\n",
        "\n",
        "    # Evaluate the model\n",
        "    vgg3.eval()\n",
        "    for data, target in dataloaders['val']:\n",
        "        if DEVICE == 'cuda':\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "        output = vgg3(data)\n",
        "        # print(output)\n",
        "        loss = criterion(output, target)\n",
        "        val_loss += loss.item()*data.size(0)\n",
        "\n",
        "        pred = torch.argmax(input = output, dim = 1)\n",
        "        correct_labels = pred.eq(target.data.view_as(pred))\n",
        "        acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "        valid_acc += acc.item() * data.size(0)\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate average losses\n",
        "    train_loss = train_loss/len(dataloaders['train'].sampler)\n",
        "    val_loss = val_loss/len(dataloaders['val'].sampler)\n",
        "    train_acc = train_acc / len(dataloaders['train'].sampler)\n",
        "    valid_acc = valid_acc / len(dataloaders['val'].sampler)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracy.append(train_acc)\n",
        "    valid_accuracy.append(valid_acc)\n",
        "    training_time.append(train_time)\n",
        "\n",
        "    # Print training/validation statistics\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch+1,\n",
        "        train_loss,\n",
        "        val_loss\n",
        "        ))\n",
        "\n",
        "\n",
        "overall_train_time = time() - overall_start_time\n",
        "print(f'Overall time taken for {epoch} including training and validation {overall_train_time}')"
      ],
      "metadata": {
        "id": "B24-ycntg7dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg3_acc_loss_time = pd.DataFrame([], columns = ['TrainingLoss','ValidationLoss', 'TrainingTime', 'TrainingAccuracy','ValidAccuracy', 'TestAccuracy','TotalParameters'  ])\n",
        "vgg3_acc_loss_time = pd.DataFrame({'TrainingLoss' : train_losses,\n",
        "                              'ValidationLoss' : val_losses,\n",
        "                              'TrainingTime' : training_time,\n",
        "                              'TrainingAccuracy' : train_accuracy,\n",
        "                              'ValidAccuracy' : valid_accuracy,\n",
        "                              'TotalParameters' : np.repeat(total_params, 10) # 10 is number of epochs\n",
        "                              })\n",
        "vgg3_acc_loss_time"
      ],
      "metadata": {
        "id": "PXo6tYO58Lm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig , ax =plt.subplots(nrows=1,ncols =2, figsize = (15,5))\n",
        "ax[0].plot( range(10), vgg3_acc_loss_time['TrainingLoss'], label= 'Training Loss')\n",
        "ax[0].plot( range(10), vgg3_acc_loss_time['ValidationLoss'], label= 'Validation Loss')\n",
        "ax[1].plot( range(10), vgg3_acc_loss_time['TrainingAccuracy'], label= 'Training Accuracy')\n",
        "ax[1].plot( range(10), vgg3_acc_loss_time['ValidAccuracy'], label= 'Validation Accuracy')\n",
        "ax[0].legend()\n",
        "ax[1].legend()"
      ],
      "metadata": {
        "id": "baVk6PPScy_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext tensorboard"
      ],
      "metadata": {
        "id": "1NdxajokZY1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %tensorboard --logdir runs/vgg3"
      ],
      "metadata": {
        "id": "TeZwsQnfZacc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = 0\n",
        "step = 0\n",
        "\n",
        "for input, target in dataloaders['test']:\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  if DEVICE == 'cuda':\n",
        "          input, target = input.cuda(), target.cuda()\n",
        "  output = vgg3(input)\n",
        "  pred = torch.argmax(input = output, dim = 1)\n",
        "  correct_labels = pred.eq(target.data.view_as(pred))\n",
        "  acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "  test_acc += acc.item()* input.size(0)\n",
        "\n",
        "  ax.imshow(input.cpu().squeeze().reshape(224,224,3))\n",
        "  ax.set_title(f'Class : {vgg3.idx_to_class[pred.item()]}')\n",
        "  ax.set_axis_off()\n",
        "\n",
        "  writer.add_scalar('Testing Accuracy / Iteration', acc, global_step = step)\n",
        "  writer.add_figure('Test images & Label', fig, global_step = step)\n",
        "  step +=1\n",
        "\n",
        "test_acc = test_acc / len(dataloaders['test'].sampler)\n",
        "vgg3_acc_loss_time['TestAccuracy'] = np.repeat(test_acc, len(vgg3_acc_loss_time))\n",
        "print(f'Testing accuracy is {test_acc}')\n"
      ],
      "metadata": {
        "id": "xcE2bGT_i-o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs/vgg3"
      ],
      "metadata": {
        "id": "4u9jznN4auSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for input, target in dataloaders['test']:\n",
        "  if DEVICE == 'cuda':\n",
        "          input, target = input.cuda(), target.cuda()\n",
        "  # print(input.shape)\n",
        "  result = vgg3(input)\n",
        "  pred_class = torch.argmax(result).item()\n",
        "  plt.title(f'Predicted class is {vgg3.idx_to_class[pred_class]}')\n",
        "  plt.imshow(input.cpu().squeeze().reshape(224,224,3))\n",
        "  plt.xlim([0,100])\n",
        "  plt.ylim([0,100])\n",
        "  break\n",
        "\n"
      ],
      "metadata": {
        "id": "OCYLThm7BG9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation"
      ],
      "metadata": {
        "id": "Pi-4v89DTi7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image transformations\n",
        "image_aug_transforms =transforms.Compose([\n",
        "        # transforms.RandomResizedCrop(size=200, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=30),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),  # Image net standards\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])  # Imagenet standards\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "3Vf8gtyNTkfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets from each folder\n",
        "images_aug = {\n",
        "    'train':\n",
        "    datasets.ImageFolder(root=traindir, transform= image_aug_transforms ),\n",
        "    'test':\n",
        "    datasets.ImageFolder(root=testdir, transform= simple_transform)\n",
        "}\n",
        "\n",
        "batch_size = 1\n",
        "# Dataloader iterators\n",
        "aug_dataloaders = {\n",
        "    'train': DataLoader(torch.utils.data.Subset(images_aug['train'], range(0, 140)), batch_size=batch_size, shuffle=True),\n",
        "    'val' : DataLoader(torch.utils.data.Subset(images_aug['train'], range(140, 160)), batch_size=batch_size, shuffle=True),\n",
        "    'test': DataLoader(images_aug['test'], batch_size=batch_size, shuffle=True)\n",
        "}"
      ],
      "metadata": {
        "id": "-14A1GXgT4wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for image, label in aug_dataloaders['train']:\n",
        "#   print(image.shape)\n",
        "#   plt.imshow(image.squeeze().reshape(224, 224, 3))\n",
        "#   break"
      ],
      "metadata": {
        "id": "bGDScm4SUGjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg3_aug = define_model()"
      ],
      "metadata": {
        "id": "fs-FoxfZanFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in vgg3_aug.parameters())\n",
        "total_params"
      ],
      "metadata": {
        "id": "dbrAp4xIST-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEVICE == 'cuda':\n",
        "  vgg3_aug.to('cuda')"
      ],
      "metadata": {
        "id": "augID0rhZm28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg3_aug.class_to_idx = images_aug['train'].class_to_idx\n",
        "vgg3_aug.idx_to_class = {idx : cls for cls,idx in images_aug['train'].class_to_idx.items()}"
      ],
      "metadata": {
        "id": "VLyOB7NuiPu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vgg3_aug.parameters())\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracy = []\n",
        "valid_accuracy = []\n",
        "training_time = []\n",
        "\n",
        "writer1 = SummaryWriter(f'runs/vgg3_aug/')\n",
        "step = 0\n",
        "overall_start_time = time()\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    train_acc = 0\n",
        "    valid_acc = 0\n",
        "\n",
        "    # Train the model\n",
        "    vgg3_aug.train()\n",
        "    start_time = time()\n",
        "    for data, target in aug_dataloaders['train']:\n",
        "        if DEVICE == 'cuda':\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "        # print(target.shape) #torch.Size([160])\n",
        "        # print(data.shape) #torch.Size([160, 3, 224, 224])\n",
        "        optimizer.zero_grad()\n",
        "        output = vgg3_aug(data) # output has probabilities of each class while the target have index of the class that is why class to index mapping is important\n",
        "        # print(output.shape) #torch.Size([160, 2])\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "        pred = torch.argmax(input = output, dim = 1)\n",
        "        correct_labels = pred.eq(target.data.view_as(pred))\n",
        "        acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "        train_acc += acc.item() * data.size(0)\n",
        "\n",
        "        writer1.add_scalar('Trainig Loss/ Iteration', loss, global_step = step)\n",
        "        writer1.add_scalar('Training Accuracy / Iteration', acc, global_step = step)\n",
        "        step += 1\n",
        "\n",
        "    train_time = time() - start_time\n",
        "    print(f'Training time in {epoch+1} is {train_time}\\n')\n",
        "\n",
        "    # Evaluate the model\n",
        "    vgg3_aug.eval()\n",
        "    for data, target in aug_dataloaders['val']:\n",
        "        if DEVICE == 'cuda':\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "        output = vgg3_aug(data)\n",
        "        # print(output)\n",
        "        loss = criterion(output, target)\n",
        "        val_loss += loss.item()*data.size(0)\n",
        "\n",
        "        pred = torch.argmax(input = output, dim = 1)\n",
        "        correct_labels = pred.eq(target.data.view_as(pred))\n",
        "        acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "        valid_acc += acc.item() * data.size(0)\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate average losses\n",
        "    train_loss = train_loss/len(aug_dataloaders['train'].sampler)\n",
        "    val_loss = val_loss/len(aug_dataloaders['val'].sampler)\n",
        "    train_acc = train_acc / len(aug_dataloaders['train'].sampler)\n",
        "    valid_acc = valid_acc / len(aug_dataloaders['val'].sampler)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracy.append(train_acc)\n",
        "    valid_accuracy.append(valid_acc)\n",
        "    training_time.append(train_time)\n",
        "\n",
        "    # Print training/validation statistics\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch+1,\n",
        "        train_loss,\n",
        "        val_loss\n",
        "        ))\n",
        "\n",
        "\n",
        "overall_train_time = time() - overall_start_time\n",
        "print(f'Overall time taken for {epoch} including training and validation {overall_train_time}')"
      ],
      "metadata": {
        "id": "bFxBF2ExUWGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg3_aug_acc_loss_time = pd.DataFrame([], columns = ['TrainingLoss', 'ValidationLoss', 'TrainingTime', 'TrainingAccuracy','ValidAccuracy', 'TestAccuracy', 'TotalParameters'  ])\n",
        "vgg3_aug_acc_loss_time = pd.DataFrame({'TrainingLoss' : train_losses,\n",
        "                              'ValidationLoss' : val_losses,\n",
        "                              'TrainingTime' : training_time,\n",
        "                              'TrainingAccuracy' : train_accuracy,\n",
        "                              'ValidAccuracy' : valid_accuracy,\n",
        "                              'TotalParameters' : np.repeat(total_params, 10)\n",
        "                              })\n",
        "vgg3_aug_acc_loss_time"
      ],
      "metadata": {
        "id": "YwGasCjiWHWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig , ax =plt.subplots(nrows=1,ncols =2, figsize = (15,5))\n",
        "ax[0].plot( range(10), vgg3_aug_acc_loss_time['TrainingLoss'], label= 'Training Loss')\n",
        "ax[0].plot( range(10), vgg3_aug_acc_loss_time['ValidationLoss'], label= 'Validation Loss')\n",
        "ax[1].plot( range(10), vgg3_aug_acc_loss_time['TrainingAccuracy'], label= 'Training Accuracy')\n",
        "ax[1].plot( range(10), vgg3_aug_acc_loss_time['ValidAccuracy'], label= 'Validation Accuracy')\n",
        "ax[0].legend()\n",
        "ax[1].legend()"
      ],
      "metadata": {
        "id": "H-YEYwYobBUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = 0\n",
        "step = 0\n",
        "for input, target in aug_dataloaders['test']:\n",
        "  fig, ax = plt.subplots()\n",
        "  if DEVICE == 'cuda':\n",
        "          input, target = input.cuda(), target.cuda()\n",
        "  output = vgg3_aug(input)\n",
        "  pred = torch.argmax(input = output, dim = 1)\n",
        "  correct_labels = pred.eq(target.data.view_as(pred))\n",
        "  acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "  test_acc += acc.item()* input.size(0)\n",
        "\n",
        "  ax.imshow(input.cpu().squeeze().reshape(224,224,3))\n",
        "  ax.set_title(f'Class : {vgg3_aug.idx_to_class[pred_class]}')\n",
        "  ax.set_axis_off()\n",
        "\n",
        "  writer1.add_scalar('Testing Accuracy / Iteration', acc, global_step = step)\n",
        "  writer1.add_figure('Test images & Label', fig, global_step = step)\n",
        "  step +=1\n",
        "\n",
        "test_acc = test_acc / len(dataloaders['test'].sampler)\n",
        "vgg3_aug_acc_loss_time['TestAccuracy'] = np.repeat(test_acc, len(vgg3_aug_acc_loss_time))\n",
        "print(f'Testing accuracy is {test_acc}')"
      ],
      "metadata": {
        "id": "IwAW4a6mWH7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs/vgg3_aug"
      ],
      "metadata": {
        "id": "vtMM5SvLb81B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for input, target in aug_dataloaders['test']:\n",
        "  if DEVICE == 'cuda':\n",
        "          input, target = input.cuda(), target.cuda()\n",
        "  # print(input.shape)\n",
        "  result = vgg3_aug(input)\n",
        "  # print(result)\n",
        "  prob = F.softmax(result, dim = 1)\n",
        "  # print(prob)\n",
        "  pred_class = torch.argmax(prob).item()\n",
        "  plt.title(f'Predicted class is {vgg3_aug.idx_to_class[pred_class]}')\n",
        "  plt.imshow(input.cpu().squeeze().reshape(224,224,3))\n",
        "  break\n",
        "\n"
      ],
      "metadata": {
        "id": "8U5ju55VWP54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Training fully connected layer"
      ],
      "metadata": {
        "id": "jgum06TDlOY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16 = models.vgg16(pretrained=True)\n"
      ],
      "metadata": {
        "id": "0jM-MvZrWSAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 2)\n",
        "for params in vgg16.parameters():\n",
        "  params.requires_grad = False\n",
        "\n",
        "for params in vgg16.classifier.parameters():\n",
        "  params.requires_grad = True"
      ],
      "metadata": {
        "id": "i2D2lPAFlNRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEVICE == 'cuda':\n",
        "  vgg16.to('cuda')"
      ],
      "metadata": {
        "id": "GWb9ETnyZ-E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16.class_to_idx = images['train'].class_to_idx\n",
        "vgg16.idx_to_class = {idx : cls for cls , idx in images['train'].class_to_idx.items()}\n",
        "vgg16.idx_to_class"
      ],
      "metadata": {
        "id": "OAV-QTRGkfc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_parameters = sum(params.numel() for params in vgg16.parameters())\n",
        "print(f'Total Parameters {total_parameters}')\n",
        "\n",
        "trainable_parameters = sum(params.numel() for params in vgg16.parameters() if params.requires_grad)\n",
        "print(f'Trainable Parameters {trainable_parameters}')"
      ],
      "metadata": {
        "id": "G6bbSmfqlUn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vgg16.parameters())\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracy = []\n",
        "valid_accuracy = []\n",
        "training_time = []\n",
        "\n",
        "writer2 = SummaryWriter(f'runs/vgg16/')\n",
        "step = 0\n",
        "overall_start_time = time()\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    train_acc = 0\n",
        "    valid_acc = 0\n",
        "\n",
        "    # Train the model\n",
        "    vgg16.train()\n",
        "    start_time = time()\n",
        "    for data, target in dataloaders['train']:\n",
        "        if DEVICE == 'cuda':\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "        # print(target.shape) #torch.Size([160])\n",
        "        # print(data.shape) #torch.Size([160, 3, 224, 224])\n",
        "        optimizer.zero_grad()\n",
        "        output = vgg16(data) # output has probabilities of each class while the target have index of the class that is why class to index mapping is important\n",
        "        # print(output.shape) #torch.Size([160, 2])\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "        pred = torch.argmax(input = output, dim = 1)\n",
        "        correct_labels = pred.eq(target.data.view_as(pred))\n",
        "        acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "        train_acc += acc.item() * data.size(0)\n",
        "\n",
        "        writer2.add_scalar('Training Accuracy/ Iteration', acc, global_step = step)\n",
        "        writer2.add_scalar('Training Loss/ Iteration', loss, global_step = step)\n",
        "        step += 1\n",
        "\n",
        "\n",
        "    train_time = time() - start_time\n",
        "    print(f'Training time in {epoch+1} is {train_time}\\n')\n",
        "\n",
        "    # Evaluate the model\n",
        "    vgg16.eval()\n",
        "    for data, target in dataloaders['val']:\n",
        "        if DEVICE == 'cuda':\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        output = vgg16(data)\n",
        "        # print(output)\n",
        "        loss = criterion(output, target)\n",
        "        val_loss += loss.item()*data.size(0)\n",
        "\n",
        "        pred = torch.argmax(input = output, dim = 1)\n",
        "        correct_labels = pred.eq(target.data.view_as(pred))\n",
        "        acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "        valid_acc += acc.item() * data.size(0)\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate average losses\n",
        "    train_loss = train_loss/len(dataloaders['train'].sampler)\n",
        "    val_loss = val_loss/len(dataloaders['val'].sampler)\n",
        "    train_acc = train_acc / len(dataloaders['train'].sampler)\n",
        "    valid_acc = valid_acc / len(dataloaders['val'].sampler)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracy.append(train_acc)\n",
        "    valid_accuracy.append(valid_acc)\n",
        "    training_time.append(train_time)\n",
        "\n",
        "    # Print training/validation statistics\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch+1,\n",
        "        train_loss,\n",
        "        val_loss\n",
        "        ))\n",
        "\n",
        "\n",
        "overall_train_time = time() - overall_start_time\n",
        "print(f'Overall time taken for {epoch} including training and validation {overall_train_time}')"
      ],
      "metadata": {
        "id": "T66X5AmCnYW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_acc_loss_time = pd.DataFrame([], columns = ['TrainingLoss','ValidationLoss', 'TrainingTime', 'TrainingAccuracy','ValidAccuracy', 'TestAccuracy','TotalParameters'  ])\n",
        "vgg16_acc_loss_time = pd.DataFrame({'TrainingLoss' : train_losses,\n",
        "                              'ValidationLoss' : val_losses,\n",
        "                              'TrainingTime' : training_time,\n",
        "                              'TrainingAccuracy' : train_accuracy,\n",
        "                              'ValidAccuracy' : valid_accuracy,\n",
        "                              'TotalParameters' : np.repeat(total_parameters, 10)# 10 is no epoch\n",
        "                              })\n",
        "vgg16_acc_loss_time\n"
      ],
      "metadata": {
        "id": "PXK3dZ3snbpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig , ax =plt.subplots(nrows=1,ncols =2, figsize = (15,5))\n",
        "ax[0].plot( range(10), vgg16_acc_loss_time['TrainingLoss'], label= 'Training Loss')\n",
        "ax[0].plot( range(10), vgg16_acc_loss_time['ValidationLoss'], label= 'Validation Loss')\n",
        "ax[1].plot( range(10), vgg16_acc_loss_time['TrainingAccuracy'], label= 'Training Accuracy')\n",
        "ax[1].plot( range(10), vgg16_acc_loss_time['ValidAccuracy'], label= 'Validation Accuracy')\n",
        "ax[0].legend()\n",
        "ax[1].legend()"
      ],
      "metadata": {
        "id": "VbJ0XqgnqnTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = 0\n",
        "step = 0\n",
        "for input, target in dataloaders['test']:\n",
        "  fig, ax = plt.subplots()\n",
        "  if DEVICE == 'cuda':\n",
        "          input, target = input.cuda(), target.cuda()\n",
        "  output = vgg16(input)\n",
        "  pred = torch.argmax(input = output, dim = 1)\n",
        "  correct_labels = pred.eq(target.data.view_as(pred))\n",
        "  acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "  test_acc += acc.item()* input.size(0)\n",
        "\n",
        "  ax.imshow(input.cpu().squeeze().reshape(224,224,3))\n",
        "  ax.set_title(f'Class : {vgg16.idx_to_class[pred_class]}')\n",
        "  ax.set_axis_off()\n",
        "\n",
        "  writer2.add_scalar('Testing Accuracy / Iteration', acc, global_step = step)\n",
        "  writer2.add_figure('Test images & Label', fig, global_step = step)\n",
        "  step +=1\n",
        "\n",
        "test_acc = test_acc / len(dataloaders['test'].sampler)\n",
        "vgg16_acc_loss_time['TestAccuracy'] = np.repeat(test_acc, len(vgg16_acc_loss_time))\n",
        "print(f'Testing accuracy is {test_acc}')"
      ],
      "metadata": {
        "id": "RMpdIn-Vqw5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs/vgg16/"
      ],
      "metadata": {
        "id": "uc-7Zc8aeLyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for input, target in dataloaders['test']:\n",
        "  if DEVICE == 'cuda':\n",
        "          input, target = input.cuda(), target.cuda()\n",
        "  # print(input.shape)\n",
        "  result = vgg16(input)\n",
        "  # print(result)\n",
        "  prob = F.softmax(result, dim = 1)\n",
        "  # print(prob)\n",
        "  pred_class = torch.argmax(prob).item()\n",
        "  plt.title(f'Predicted class is {vgg16.idx_to_class[pred_class]}')\n",
        "  plt.imshow(input.cpu().squeeze().reshape(224,224,3))\n",
        "  break\n",
        "\n"
      ],
      "metadata": {
        "id": "zVzBIOIcrEOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training entire vgg16"
      ],
      "metadata": {
        "id": "PgHkmWJXF95W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_entire = models.vgg16(pretrained=True)"
      ],
      "metadata": {
        "id": "BpRkzI22GMdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEVICE == 'cuda':\n",
        "  vgg16_entire = vgg16_entire.to('cuda')"
      ],
      "metadata": {
        "id": "YOKTXafpYHiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_entire.class_to_idx = images['train'].class_to_idx\n",
        "vgg16_entire.idx_to_class = {idx : cls for cls , idx in images['train'].class_to_idx.items()}\n",
        "vgg16_entire.idx_to_class"
      ],
      "metadata": {
        "id": "MI8j6BtsEq8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_parameters = sum(params.numel() for params in vgg16_entire.parameters())\n",
        "print(f'Total Parameters {total_parameters}')\n",
        "\n",
        "trainable_parameters = sum(params.numel() for params in vgg16_entire.parameters() if params.requires_grad)\n",
        "print(f'Trainable Parameters {trainable_parameters}')"
      ],
      "metadata": {
        "id": "sVKslNT2GORi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vgg16_entire.parameters())\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracy = []\n",
        "valid_accuracy = []\n",
        "training_time = []\n",
        "\n",
        "steps = 0\n",
        "writer3 = SummaryWriter(f'runs/vgg16_entire/')\n",
        "overall_start_time = time()\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    train_acc = 0\n",
        "    valid_acc = 0\n",
        "\n",
        "    # Train the model\n",
        "    vgg16_entire.train()\n",
        "    start_time = time()\n",
        "    for data, target in dataloaders['train']:\n",
        "        if DEVICE == 'cuda':\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "        # print(target.shape) #torch.Size([160])\n",
        "        # print(data.shape) #torch.Size([160, 3, 224, 224])\n",
        "        optimizer.zero_grad()\n",
        "        output = vgg16_entire(data) # output has probabilities of each class while the target have index of the class that is why class to index mapping is important\n",
        "        # print(output.shape) #torch.Size([160, 2])\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "        pred = torch.argmax(input = output, dim = 1)\n",
        "        correct_labels = pred.eq(target.data.view_as(pred))\n",
        "        acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "        train_acc += acc.item() * data.size(0)\n",
        "\n",
        "        writer3.add_scalar('Training Accuracy/Iteration', acc, global_step = step)\n",
        "        writer3.add_scalar('Training Loss/Iteration', loss, global_step = step)\n",
        "        step += 1\n",
        "\n",
        "    train_time = time() - start_time\n",
        "    print(f'Training time in {epoch+1} is {train_time}\\n')\n",
        "\n",
        "    # Evaluate the model\n",
        "    vgg16_entire.eval()\n",
        "    for data, target in dataloaders['val']:\n",
        "        if DEVICE == 'cuda':\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "        output = vgg16_entire(data)\n",
        "        # print(output)\n",
        "        loss = criterion(output, target)\n",
        "        val_loss += loss.item()*data.size(0)\n",
        "\n",
        "        pred = torch.argmax(input = output, dim = 1)\n",
        "        correct_labels = pred.eq(target.data.view_as(pred))\n",
        "        acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "        valid_acc += acc.item() * data.size(0)\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate average losses\n",
        "    train_loss = train_loss/len(dataloaders['train'].sampler)\n",
        "    val_loss = val_loss/len(dataloaders['val'].sampler)\n",
        "    train_acc = train_acc / len(dataloaders['train'].sampler)\n",
        "    valid_acc = valid_acc / len(dataloaders['val'].sampler)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracy.append(train_acc)\n",
        "    valid_accuracy.append(valid_acc)\n",
        "    training_time.append(train_time)\n",
        "\n",
        "    # Print training/validation statistics\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch+1,\n",
        "        train_loss,\n",
        "        val_loss\n",
        "        ))\n",
        "\n",
        "\n",
        "overall_train_time = time() - overall_start_time\n",
        "print(f'Overall time taken for {epoch} including training and validation {overall_train_time}')"
      ],
      "metadata": {
        "id": "L-OYmQTxGa57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_entire_acc_loss_time = pd.DataFrame([], columns = ['TrainingLoss','ValidationLoss', 'TrainingTime', 'TrainingAccuracy','ValidAccuracy', 'TestAccuracy','TotalParameters'  ])\n",
        "vgg16_entire_acc_loss_time = pd.DataFrame({'TrainingLoss' : train_losses,\n",
        "                              'ValidationLoss' : val_losses,\n",
        "                              'TrainingTime' : training_time,\n",
        "                              'TrainingAccuracy' : train_accuracy,\n",
        "                              'ValidAccuracy' : valid_accuracy,\n",
        "                              'TotalParameters' : np.repeat(total_parameters, 10)\n",
        "                              })\n",
        "vgg16_entire_acc_loss_time\n"
      ],
      "metadata": {
        "id": "pgk5W-1AGpck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig , ax =plt.subplots(nrows=1,ncols =2, figsize = (15,5))\n",
        "ax[0].plot( range(10), vgg16_entire_acc_loss_time['TrainingLoss'], label= 'Training Loss')\n",
        "ax[0].plot( range(10), vgg16_entire_acc_loss_time['ValidationLoss'], label= 'Validation Loss')\n",
        "ax[1].plot( range(10), vgg16_entire_acc_loss_time['TrainingAccuracy'], label= 'Training Accuracy')\n",
        "ax[1].plot( range(10), vgg16_entire_acc_loss_time['ValidAccuracy'], label= 'Validation Accuracy')\n",
        "ax[0].legend()\n",
        "ax[1].legend()"
      ],
      "metadata": {
        "id": "QNJp9uZCG1Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = 0\n",
        "step = 0\n",
        "for input, target in dataloaders['test']:\n",
        "  fig, ax = plt.subplots()\n",
        "  if DEVICE == 'cuda':\n",
        "          input, target = input.cuda(), target.cuda()\n",
        "  output = vgg16_entire(input)\n",
        "  pred = torch.argmax(input = output, dim = 1)\n",
        "  correct_labels = pred.eq(target.data.view_as(pred))\n",
        "  acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "  test_acc += acc.item()* input.size(0)\n",
        "\n",
        "  ax.imshow(input.cpu().squeeze().reshape(224,224,3))\n",
        "  ax.set_title(f'Class : {vgg16_entire.idx_to_class[pred_class]}')\n",
        "  ax.set_axis_off()\n",
        "\n",
        "  writer3.add_scalar('Testing Accuracy / Iteration', acc, global_step = step)\n",
        "  writer3.add_figure('Test images & Label', fig, global_step = step)\n",
        "  step +=1\n",
        "\n",
        "test_acc = test_acc / len(dataloaders['test'].sampler)\n",
        "vgg16_entire_acc_loss_time['TestAccuracy'] = np.repeat(test_acc, len(vgg16_entire_acc_loss_time))\n",
        "print(f'Testing accuracy is {test_acc}')"
      ],
      "metadata": {
        "id": "UOf2lIjfG7dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs/vgg16_entire/"
      ],
      "metadata": {
        "id": "YxQ4d4LPfYG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for input, target in dataloaders['test']:\n",
        "  if DEVICE == 'cuda':\n",
        "          input, target = input.cuda(), target.cuda()\n",
        "  # print(input.shape)\n",
        "  result = vgg16_entire(input)\n",
        "  # print(result)\n",
        "  prob = F.softmax(result, dim = 1)\n",
        "  # print(prob)\n",
        "  pred_class = torch.argmax(prob).item()\n",
        "  plt.title(f'Predicted class is {vgg16_entire.idx_to_class[pred_class]}')\n",
        "  plt.imshow(input.cpu().squeeze().reshape(224,224,3))\n",
        "  break\n",
        "\n"
      ],
      "metadata": {
        "id": "aTDaV1pOG9q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training MLP"
      ],
      "metadata": {
        "id": "B6eGrSlfcoYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLP = nn.Sequential(\n",
        "    nn.Linear(150528, 900),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(900, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256,128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128,2)\n",
        ")\n",
        "if DEVICE == 'cuda':\n",
        "  MLP.to('cuda')"
      ],
      "metadata": {
        "id": "azEoZz8ia6Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in MLP.parameters())\n",
        "total_params"
      ],
      "metadata": {
        "id": "QmMC0SQcT4Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLP.class_to_idx = images['train'].class_to_idx\n",
        "MLP.idx_to_class = {idx : cls for cls, idx in images['train'].class_to_idx.items()}"
      ],
      "metadata": {
        "id": "dqFwBWBc0Jp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(MLP, input_size= (150528,))"
      ],
      "metadata": {
        "id": "STfZuUpMc3dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(MLP.parameters())\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracy = []\n",
        "valid_accuracy = []\n",
        "training_time = []\n",
        "\n",
        "overall_start_time = time()\n",
        "step = 0\n",
        "writer4 = SummaryWriter(f'runs/MLP/')\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    train_acc = 0\n",
        "    valid_acc = 0\n",
        "\n",
        "    # Train the model\n",
        "    MLP.train()\n",
        "    start_time = time()\n",
        "    for data, target in dataloaders['train']:\n",
        "        if DEVICE == 'cuda':\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "        # data = data.view(-1) #flatten the image\n",
        "        # print(target)\n",
        "        # print(target.shape) #torch.Size([160])\n",
        "        # print(data.shape) #torch.Size([160, 3, 224, 224])\n",
        "        optimizer.zero_grad()\n",
        "        output = MLP(data.view(-1)) # output has probabilities of each class while the target have index of the class that is why class to index mapping is important\n",
        "        output = output.unsqueeze(dim = 0)\n",
        "        # print(output)\n",
        "        # print(output.shape) #torch.Size([160, 2])\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "        pred = torch.argmax(input = output, dim = 1)\n",
        "        correct_labels = pred.eq(target.data.view_as(pred))\n",
        "        acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "        # print(f'accuarcy per batch {acc}')\n",
        "        train_acc += acc.item() * data.size(0)\n",
        "        # print(data.size(0))\n",
        "        # print(f'combined accuracy per batch {train_acc}')\n",
        "        writer4.add_scalar('Training Accuracy / Iteration', acc, global_step = step)\n",
        "        writer4.add_scalar('Training Loss / Iteration', loss, global_step = step)\n",
        "        step += 1\n",
        "\n",
        "    train_time = time() - start_time\n",
        "    print(f'Training time in {epoch+1} is {train_time}\\n')\n",
        "\n",
        "    # Evaluate the model\n",
        "    MLP.eval()\n",
        "    for data, target in dataloaders['val']:\n",
        "        if DEVICE == 'cuda':\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "        # data = data.view(-1)\n",
        "        output = MLP(data.view(-1))\n",
        "        output = output.unsqueeze(dim = 0)\n",
        "        # print(output)\n",
        "        loss = criterion(output, target)\n",
        "        val_loss += loss.item()*data.size(0)\n",
        "\n",
        "        pred = torch.argmax(input = output, dim = 1)\n",
        "        correct_labels = pred.eq(target.data.view_as(pred))\n",
        "        acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "        valid_acc += acc.item() * data.size(0)\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate average losses\n",
        "    train_loss = train_loss/len(dataloaders['train'].sampler)\n",
        "    val_loss = val_loss/len(dataloaders['val'].sampler)\n",
        "    train_acc = train_acc / len(dataloaders['train'].sampler)\n",
        "    print(f'Accuracy per epoch {train_acc}')\n",
        "    valid_acc = valid_acc / len(dataloaders['val'].sampler)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracy.append(train_acc)\n",
        "    valid_accuracy.append(valid_acc)\n",
        "    training_time.append(train_time)\n",
        "\n",
        "    # Print training/validation statistics\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch+1,\n",
        "        train_loss,\n",
        "        val_loss\n",
        "        ))\n",
        "\n",
        "\n",
        "overall_train_time = time() - overall_start_time\n",
        "print(f'Overall time taken for {n_epochs} including training and validation {overall_train_time}')"
      ],
      "metadata": {
        "id": "CTnLQbEQdCGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_acc_loss_time = pd.DataFrame([], columns = ['TrainingLoss','ValidationLoss', 'TrainingTime', 'TrainingAccuracy','ValidAccuracy', 'TestAccuracy','TotalParameters'  ])\n",
        "mlp_acc_loss_time = pd.DataFrame({'TrainingLoss' : train_losses,\n",
        "                              'ValidationLoss' : val_losses,\n",
        "                              'TrainingTime' : training_time,\n",
        "                              'TrainingAccuracy' : train_accuracy,\n",
        "                              'ValidAccuracy' : valid_accuracy,\n",
        "                              'TotalParameters' : np.repeat(total_params, 10) # 10 is epochs\n",
        "                              })\n",
        "mlp_acc_loss_time\n"
      ],
      "metadata": {
        "id": "n4HfKfb2ef3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig , ax =plt.subplots(nrows=1,ncols =2, figsize = (15,5))\n",
        "ax[0].plot( range(10), mlp_acc_loss_time['TrainingLoss'], label= 'Training Loss')\n",
        "ax[0].plot( range(10), mlp_acc_loss_time['ValidationLoss'], label= 'Validation Loss')\n",
        "ax[1].plot( range(10), mlp_acc_loss_time['TrainingAccuracy'], label= 'Training Accuracy')\n",
        "ax[1].plot( range(10), mlp_acc_loss_time['ValidAccuracy'], label= 'Validation Accuracy')\n",
        "ax[0].legend()\n",
        "ax[1].legend()"
      ],
      "metadata": {
        "id": "SAzQc1V_e3XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = 0\n",
        "step = 0\n",
        "for input, target in dataloaders['test']:\n",
        "  fig, ax = plt.subplots()\n",
        "  if DEVICE == 'cuda':\n",
        "          input, target = input.cuda(), target.cuda()\n",
        "  # input = input.view(-1)\n",
        "  output = MLP(input.view(-1))\n",
        "  output = output.unsqueeze(dim = 0)\n",
        "  pred = torch.argmax(input = output, dim = 1)\n",
        "  correct_labels = pred.eq(target.data.view_as(pred))\n",
        "  acc = torch.mean(correct_labels.type(torch.FloatTensor))\n",
        "  test_acc += acc.item()* input.size(0)\n",
        "\n",
        "  ax.imshow(input.cpu().squeeze().reshape(224,224,3))\n",
        "  ax.set_title(f'Class : {MLP.idx_to_class[pred_class]}')\n",
        "  ax.set_axis_off()\n",
        "\n",
        "  writer4.add_scalar('Testing Accuracy / Iteration', acc, global_step = step)\n",
        "  writer4.add_figure('Test images & Label', fig, global_step = step)\n",
        "  step +=1\n",
        "\n",
        "test_acc = test_acc / len(dataloaders['test'].sampler)\n",
        "mlp_acc_loss_time['TestAccuracy'] = np.repeat(test_acc, len(mlp_acc_loss_time))\n",
        "print(f'Testing accuracy is {test_acc}')"
      ],
      "metadata": {
        "id": "zJv-Ucure7PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs/MLP/"
      ],
      "metadata": {
        "id": "zQhyJmVvghwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input, target in dataloaders['test']:\n",
        "  if DEVICE == 'cuda':\n",
        "          input, target = input.cuda(), target.cuda()\n",
        "  # input = input.view(-1)\n",
        "  # print(input.shape)\n",
        "  result = MLP(input.view(-1))\n",
        "  result = result.unsqueeze(dim = 0)\n",
        "  # print(result)\n",
        "  prob = F.softmax(result, dim = 1)\n",
        "  # print(prob)\n",
        "  pred_class = torch.argmax(prob).item()\n",
        "  plt.title(f'Predicted class is {MLP.idx_to_class[pred_class]}')\n",
        "  plt.imshow(input.cpu().squeeze().reshape(224,224,3))\n",
        "  break\n",
        "\n"
      ],
      "metadata": {
        "id": "kLWf7BwXfPmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_table = pd.concat([vgg1_acc_loss_time.mean(),vgg3_acc_loss_time.mean(), vgg3_aug_acc_loss_time.mean(),vgg16_acc_loss_time.mean(),vgg16_entire_acc_loss_time.mean(),mlp_acc_loss_time.mean()], axis = 1).T\n",
        "combined_table.index = ['VGG1','VGG3','VGG3_augmentation','VGG16_FC','VGG16_entire','MLP']\n",
        "combined_table"
      ],
      "metadata": {
        "id": "Y2LXw5N5RlJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Are the results as expected? Why or why not?\n",
        "\n"
      ],
      "metadata": {
        "id": "WRlZ1S3wpV0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG3 consists of 3 blocks of convolutional layer and pooling layer. These are trained on the images with random weights initially. It has good training accuracy but the test accuracy is small. This tells that the training dataset should be large so that the model generalises well.\n"
      ],
      "metadata": {
        "id": "3JXJbTjUp8OR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Does data augmentation help? Why or why not?\n",
        "\n"
      ],
      "metadata": {
        "id": "tp_zC_6Npzva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation helps the model to see variations in the training images and it also increases thr training dataset but the number of epochs should be increased so that the model learns the features from large dataset appropriately."
      ],
      "metadata": {
        "id": "LjqERRM7rlL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Does it matter how many epochs you fine tune the model? Why or why not?\n",
        "\n"
      ],
      "metadata": {
        "id": "diKGK2cHp2np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, in case of VGG16_FC, only fully connected layer is fine tuned and in the convolutional layer, pre trained weights are used. This involves less number of parameters training and also the pretrained weights of convolutional layer are good enough to capture the features of the images such that fully connected layer gets right information from its previous layers. Hence, convergence occurs faster and less number of epochs are needed when compared to training the entire VGG network"
      ],
      "metadata": {
        "id": "ZS4kyqMFsDjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Are there any particular images that the model is confused about? Why or why not?"
      ],
      "metadata": {
        "id": "tRk-87Mrp46T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there are images in which the model is confused."
      ],
      "metadata": {
        "id": "DjzxBUdNu_lC"
      }
    }
  ]
}